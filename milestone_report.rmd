---
title: "Coursera Data Science Capstone Project MileStone Report"
author: "Kevin Dyke"
date: "29 April 2016"
output: html_document
---

##Aims
Does the link lead to an HTML page describing the exploratory analysis of the training data set?
Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?
Has the data scientist made basic plots, such as histograms to illustrate features of the data?
Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?

```{r Load_Libraries}
library(stringi)
library(knitr)
library(tm)
library(SnowballC)
library(ggplot2)
library(RWeka)
library(RWekajars)
```

## Getting the data 

```{r get_data}
# Download the required datasets and unzip 
if (!file.exists("Coursera-SwiftKey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",destfile = "C:\\coursera\\Data Science\\Coursera-Data-Science-Capstone-Project\\Coursera-SwiftKey.zip")
  unzip("Coursera-SwiftKey.zip")
}

if(!file.exists("bannedWords.zip")){
download.file("http://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-banned-by-google-txt-file.zip","bannedWords.zip");
unzip("bannedWords.zip");
}
```

```{r read_data}

mb <- 1024 * 1024

blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)


# Get file sizes
blogsSize <- round(file.info("final/en_US/en_US.blogs.txt")$size / mb,2)
newsSize <- round(file.info("final/en_US/en_US.news.txt")$size / mb ,2)
twitterSize <- round(file.info("final/en_US/en_US.twitter.txt")$size /mb ,2)

# Get words in files
blogsWords <- stri_count_words(blogs)
newsWords <- stri_count_words(news)
twitterWords <- stri_count_words(twitter)

# Summary of the data sets
df <- data.frame(source = c("Blogs", "News", "Twitter"),
           size = c(blogsSize, newsSize, twitterSize),
           lines = c(length(blogs), length(news), length(twitter)),
           words = c(sum(blogsWords), sum(newsWords), sum(twitterWords)))

colnames(df) <- c("Source", "Size (MB)", "Number of Lines","Number of Words")
kable(df,caption="A test table", 
      align = c("c"))
```

## Sample


```{r corpus}
set.seed(2566)  # Ensure reproducibility 

sampleSize <- 0.1 # set sample size to 1%
docs <- c(sample(blogs, length(blogs) * sampleSize),
                 sample(news, length(news) * sampleSize),
                 sample(twitter, length(twitter) * sampleSize))

# convert string to vector of words
dat2 <- unlist(strsplit(docs, split=", "))
# find indices of words with non-ASCII characters
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
# subset original vector of words to exclude words with non-ASCII char
dat4 <- dat2[-dat3]
# convert vector back to a string
docs <- paste(dat4, collapse = ", ")
  
corpus <- Corpus(VectorSource(docs))
inspect(corpus)
rm(blogs, news, twitter)
rm(docs)
```


```{r shape_corpus}

# clean up the corpus (function calls are obvious)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
 
# remove common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

bannedWords <- readLines("full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt", warn = FALSE)
corpus <- tm_map(corpus, removeWords, bannedWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)

writeCorpus(corpus, path = ".\\Corpus", filenames = "Corpus.txt")

dtm <- DocumentTermMatrix(corpus)
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
bigramDTM <- DocumentTermMatrix(corpus, control = list(tokenize = bigramTokenizer))

trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigramDTM <- DocumentTermMatrix(corpus, control = list(tokenize = trigramTokenizer))

dtm2 <- as.matrix(dtm)

frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)

head(frequency)

uniFreq <- data.frame(word = names(frequency), freq = frequency)

```

```{r Ngrams}
options(mc.cores=1)


#dtm2 <- as.matrix(bigram)

#frequency <- colSums(dtm2)
#frequency <- sort(frequency, decreasing=TRUE)

#head(frequency)

#biFreq <- data.frame(word = names(frequency), freq = frequency)
#makePlot(biFreq,20,"A LABEL","Frequency","green")




```

```{r histograms}
makePlot <- function(data, num,xLabel,yLabel,colour) {
  ggplot(data[1:num,], aes(reorder(word, -freq), freq)) +
         labs(x = xLabel, y = yLabel) +
         theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
         geom_bar(stat = "identity", fill = I(colour))
}

makePlot(uniFreq,20,"A LABEL","Frequency","green")

```

