---
title: "Coursera Data Science Capstone Project MileStone Report"
author: "Kevin Dyke"
date: "29 April 2016"
output: html_document
---

##Aims
Does the link lead to an HTML page describing the exploratory analysis of the training data set?
Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?
Has the data scientist made basic plots, such as histograms to illustrate features of the data?
Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?

```{r Load_Libraries}
library(stringi)
library(knitr)
library(tm)
library(SnowballC)
library(ggplot2)
library(RWeka)
library(RWekajars)
library(plyr)
```

## Getting the data 

```{r get_data}
# Download the required datasets and unzip 
if (!file.exists("Coursera-SwiftKey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",destfile = "C:\\coursera\\Data Science\\Coursera-Data-Science-Capstone-Project\\Coursera-SwiftKey.zip")
  unzip("Coursera-SwiftKey.zip")
}

if(!file.exists("bannedWords.zip")){
download.file("http://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-banned-by-google-txt-file.zip","bannedWords.zip");
unzip("bannedWords.zip");
}
```

```{r read_data}

mb <- 1024 * 1024

blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)


# Get file sizes
blogsSize <- round(file.info("final/en_US/en_US.blogs.txt")$size / mb,2)
newsSize <- round(file.info("final/en_US/en_US.news.txt")$size / mb ,2)
twitterSize <- round(file.info("final/en_US/en_US.twitter.txt")$size /mb ,2)

# Get words in files
blogsWords <- stri_count_words(blogs)
newsWords <- stri_count_words(news)
twitterWords <- stri_count_words(twitter)

# Summary of the data sets
df <- data.frame(source = c("Blogs", "News", "Twitter"),
           size = c(blogsSize, newsSize, twitterSize),
           lines = c(length(blogs), length(news), length(twitter)),
           words = c(sum(blogsWords), sum(newsWords), sum(twitterWords)))

colnames(df) <- c("Source", "Size (MB)", "Number of Lines","Number of Words")
kable(df,caption="A test table", 
      align = c("c"))
```

## Sample


```{r corpus}
set.seed(2566)  # Ensure reproducibility 

sampleSize <- 0.01 # set sample size to 1%
docs <- c(sample(blogs, length(blogs) * sampleSize),
                 sample(news, length(news) * sampleSize),
                 sample(twitter, length(twitter) * sampleSize))


corpus <- Corpus(VectorSource(docs))
rm(blogs, news, twitter)
rm(docs)
```


```{r shape_corpus}

# clean up the corpus (function calls are obvious)


# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))  

removeSpecial <- function(x) gsub("[^[:print:]]+", "", x)
corpus <- tm_map(corpus, content_transformer(removeSpecial))  

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
 
# remove common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

bannedWords <- readLines("full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt", warn = FALSE)
corpus <- tm_map(corpus, removeWords, bannedWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
```


```{r histograms}
makePlot <- function(data, num,xLabel,yLabel,title,colour) {
  ggplot(data[1:num,], aes(reorder(word, -freq), freq)) +
         labs(x = xLabel, y = yLabel) +
         ggtitle(title) +      
         theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
         geom_bar(stat = "identity", fill = I(colour))
}
```



```{r build_Ngrams}
N1GramTokenizer <- function(x) NGramTokenizer(x = x, control = Weka_control(min = 1, max = 1))
N2GramTokenizer <- function(x) NGramTokenizer(x = x, control = Weka_control(min = 2, max = 2))
N3GramTokenizer <- function(x) NGramTokenizer(x = x, control = Weka_control(min = 3, max = 3))
N1GramMatrix <- TermDocumentMatrix(x = corpus, control = list(tokenize = N1GramTokenizer))
N2GramMatrix <- TermDocumentMatrix(x = corpus, control = list(tokenize = N2GramTokenizer))
N3GramMatrix <- TermDocumentMatrix(x = corpus, control = list(tokenize = N3GramTokenizer))
#N1GramMatrix <- removeSparseTerms(N1GramMatrix, 0.999)
#N2GramMatrix <- removeSparseTerms(N2GramMatrix, 0.9999)
#N3GramMatrix <- removeSparseTerms(N3GramMatrix, 0.9999)
```



```{r Unigram}
## convert docmatrix to a dataframe with word and freq
df_words <- as.data.frame(slam::row_sums(N1GramMatrix, na.rm=T))
colnames(df_words)<- "freq"
df_words <- cbind(word = rownames(df_words),df_words)
rownames(df_words) <- NULL

## sort the dataframe by most used words

uniFreq <- arrange(df_words,desc(freq))
makePlot(uniFreq,20,"Words","Frequency","Top 20 Unigram Words","green")
```


```{r Bigram}
## convert docmatrix to a dataframe with word and freq
df_words <- as.data.frame(slam::row_sums(N2GramMatrix, na.rm=T))
colnames(df_words)<- "freq"
df_words <- cbind(word = rownames(df_words),df_words)
rownames(df_words) <- NULL

## sort the dataframe by most used words

BiFreq <- arrange(df_words,desc(freq))
makePlot(BiFreq,20,"Words","Frequency","Top 20 Bigram Words","blue")
```

```{r Trigram}
## convert docmatrix to a dataframe with word and freq
df_words <- as.data.frame(slam::row_sums(N3GramMatrix, na.rm=T))
colnames(df_words)<- "freq"
df_words <- cbind(word = rownames(df_words),df_words)
rownames(df_words) <- NULL

## sort the dataframe by most used words

TriFreq <- arrange(df_words,desc(freq))
makePlot(TriFreq,20,"Words","Frequency","Top 20 Trigram Words","yellow")
```

